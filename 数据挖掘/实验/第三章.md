# 第三章

##  一元线性回归

###  一元线性回归的代码实现

**1. 绘制散点图**


```python
import matplotlib.pyplot as plt
X = [[1], [2], [4], [5]]
Y = [2, 4, 6, 8]
plt.scatter(X, Y)
plt.show()
```


    <Figure size 640x480 with 1 Axes>


**2. 引入Scikit-learn库搭建模型**


```python
from sklearn.linear_model import LinearRegression
regr = LinearRegression()
regr.fit(X,Y)
```




    LinearRegression()



**3. 模型预测**


```python
# 模型预测 - 预测一个数据
y = regr.predict([[1.5]])
print(y)
```

    [2.9]



```python
# 模型预测 - 预测多个数据
y = regr.predict([[1.5], [2.5], [4.5]])
print(y)
```

    [2.9 4.3 7.1]


**4. 模型可视化**


```python
plt.scatter(X, Y)
plt.plot(X, regr.predict(X))
plt.show()
```


​    
![在这里插入图片描述](https://img-blog.csdnimg.cn/45c341373e9849d1b9b587e51f8a788e.png)



**5. 线性回归方程构造**


```python
print('系数a为:' + str(regr.coef_[0]))
print('截距b为:' + str(regr.intercept_))
```

    系数a为:1.4000000000000004
    截距b为:0.7999999999999989


那么此时的一元线性回归得到的线性回归方程就可以表示为如下形式：y = 1.4*x + 0.8

###  案例实战：不同行业工龄与薪水的线性回归模型

**1. 案例背景**

通常来说，薪水会随着工龄的增长而增长，而在不同的行业中薪水的增长速度都会有所不同，本案例通过一元线性回归模型来探寻工龄对薪水的影响，也即搭建薪水预测模型，同时比较多个行业的薪水预测模型来分析各个行业的特点。

**2. 读取数据**


```python
import pandas as pd
df = pd.read_excel('IT行业收入表.xlsx')
df.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }


    .dataframe tbody tr th {
        vertical-align: top;
    }
    
    .dataframe thead th {
        text-align: right;
    }

</style>

<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>工龄</th>
      <th>薪水</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.0</td>
      <td>10808</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.1</td>
      <td>13611</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.2</td>
      <td>12306</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.3</td>
      <td>12151</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.3</td>
      <td>13057</td>
    </tr>
  </tbody>
</table>

</div>




```python
# 此时的工龄为自变量，薪水为因变量，通过如下代码进行自变量、因变量选取
X = df[['工龄']]
Y = df['薪水']
```


```python
# 通过如下代码可以将此时的散点图绘制出来：
from matplotlib import pyplot as plt
plt.rcParams['font.sans-serif'] = ['SimHei']  # 用来正常显示中文标签
plt.scatter(X,Y)
plt.xlabel('工龄')
plt.ylabel('薪水')
plt.show()
```


​    
![在这里插入图片描述](https://img-blog.csdnimg.cn/2cbc8e7eba0b491a8ba643c2e58d39a4.png)

​    


**3. 模型搭建**


```python
from sklearn.linear_model import LinearRegression
regr = LinearRegression()  # 引入模型
regr.fit(X,Y)  # 训练模型
```




    LinearRegression()



**4. 模型可视化**


```python
plt.scatter(X,Y)
plt.plot(X, regr.predict(X), color='red')  # color='red'设置为红色
plt.xlabel('工龄')
plt.ylabel('薪水')
plt.show()
```


​    
![在这里插入图片描述](https://img-blog.csdnimg.cn/243871d8d7ca44858ba1d81b2c29a0c0.png)

​    


**5. 线性回归方程构造**


```python
print('系数a为:' + str(regr.coef_[0]))
print('截距b为:' + str(regr.intercept_))
```

    系数a为:2497.1513476046866
    截距b为:10143.131966873787


所以此时的一元线性回归曲线方程为：y = 2497*x + 10143

**补充知识点：模型优化 - 一元多次线性回归模型**

对于一元线性回归模型而言，其实它还有一个进阶版本，叫作一元多次线性回归模型，比较常见的有一元二次线性回归模型，其格式如下：

**$y = ax^2 + bx + c$**

我们之所以还会研究一元多次线性回归模型，是因为有时真正契合的趋势线可能不是一条直线，而是一条曲线，比如下图根据一元二次线性回归模型形成的曲线更契合散点图背后的趋势。

![在这里插入图片描述](https://img-blog.csdnimg.cn/e4566bf6027d4977b19e3064ca25ac00.png)



```python
# 通过如下代码生成二次项数据：
from sklearn.preprocessing import PolynomialFeatures
poly_reg = PolynomialFeatures(degree=2)
X_ = poly_reg.fit_transform(X)
```


```python
print(X_[0:5])
```

    [[1.   0.   0.  ]
     [1.   0.1  0.01]
     [1.   0.2  0.04]
     [1.   0.3  0.09]
     [1.   0.3  0.09]]



```python
# 模型训练
regr = LinearRegression()
regr.fit(X_, Y)
```




    LinearRegression()




```python
# 可视化
plt.scatter(X,Y)
plt.plot(X, regr.predict(X_), color='red')
plt.show()
```


​    
![在这里插入图片描述](https://img-blog.csdnimg.cn/201e829bf8314a00ace11da2b98fbd10.png)

​    



```python
# 打印系数和常数项
print(regr.coef_)  # 获取系数a, b 
print(regr.intercept_)  # 获取常数项c
```

    [   0.         -743.68080444  400.80398224]
    13988.159332096888


此时的系数项中为3个数，第一个0对应之前生成的X_常数项前面的系数，也对应之前说的X_的常数项不会产生影响；-743.68代表的X_一次项前面的系数，也即系数b；400.8代表的X_二次项前面的系数，也即系数a；而13988则代表常数项c，所以该一元二次线性回归方程为：

**$y = 400.8x^2 - 743.68x + 13988$**


```python
itdf = pd.read_excel('IT行业收入表.xlsx')
itX = itdf[['工龄']]
itY = itdf['薪水']
```


```python
fdf = pd.read_excel('金融行业收入表.xlsx')
fX = fdf[['工龄']]
fY = fdf['薪水']
```


```python
autodf = pd.read_excel('汽车制造行业收入表.xlsx')
autoX = autodf[['工龄']]
autoY = autodf['薪水']
```


```python
cateringdf = pd.read_excel('餐饮服务行业收入表.xlsx')
cateringX = cateringdf[['工龄']]
cateringY = cateringdf['薪水']
```


```python
poly_reg1 = PolynomialFeatures(degree=2)
itX_ = poly_reg1.fit_transform(itX)
poly_reg2 = PolynomialFeatures(degree=2)
fX_ = poly_reg2.fit_transform(fX)
poly_reg3 = PolynomialFeatures(degree=2)
autoX_ = poly_reg3.fit_transform(autoX)
poly_reg4 = PolynomialFeatures(degree=2)
cateringX_ = poly_reg4.fit_transform(cateringX)
```


```python
plt.rcParams['figure.figsize']=(12,8)
ax1 = plt.subplot(221)
regr1 = LinearRegression()
regr1.fit(itX_, itY)
plt.title("IT行业薪水")
plt.scatter(itX,itY)
plt.plot(itX, regr.predict(itX_), color='green')
plt.xlim(0,9)
plt.ylim(0,40000)
plt.grid(True)
ax2 = plt.subplot(222)
regr2 = LinearRegression()
regr2.fit(fX_, fY)
plt.title("金融行业薪水")
plt.scatter(fX,fY)
plt.plot(fX, regr2.predict(fX_), color='red')
plt.xlim(0,9)
plt.ylim(0,60000)
plt.grid(True)
ax3 = plt.subplot(223)
regr3 = LinearRegression()
regr3.fit(autoX_, autoY)
plt.title("汽车制造行业薪水")
plt.scatter(autoX,autoY)
plt.plot(autoX, regr3.predict(autoX_), color='red')
plt.xlim(0,9)
plt.ylim(0,20000)
plt.grid(True)
ax4 = plt.subplot(224)
regr4 = LinearRegression()
regr4.fit(cateringX_, cateringY)
plt.title("餐饮服务行业薪水")
plt.scatter(cateringX,cateringY)
plt.plot(cateringX, regr4.predict(cateringX_), color='red')
plt.xlim(0,9)
plt.ylim(0,14000)
plt.grid(True)
```


​    
![在这里插入图片描述](https://img-blog.csdnimg.cn/0a94cbf6f23f45d589b5d0b763c7f55b.png)

​    



```python

```
##  线性回归模型评估

###  模型评估的编程实现

3.1.3 代码汇总 ：不同行业工作年限与收入的线性回归模型


```python
# 1.读取数据
import pandas
df = pandas.read_excel('IT行业收入表.xlsx')
X = df[['工龄']]
Y = df['薪水']

# 2.模型训练
from sklearn.linear_model import LinearRegression
regr = LinearRegression()
regr.fit(X,Y)

# 3.模型可视化
from matplotlib import pyplot as plt
plt.scatter(X,Y)
plt.plot(X, regr.predict(X), color='red')  # color='red'设置为红色
plt.xlabel('工龄')
plt.ylabel('薪水')
plt.show()

# 4.线性回归方程构造
print('系数a为:' + str(regr.coef_[0]))
print('截距b为:' + str(regr.intercept_))
```


    <Figure size 640x480 with 1 Axes>


    系数a为:2497.1513476046866
    截距b为:10143.131966873787



```python
import statsmodels.api as sm
X2 = sm.add_constant(X)
est = sm.OLS(Y, X2).fit()
est.summary()  # 在非Jupyter Notebook的编辑器中需要写成print(est.summary())
```

    C:\Users\LYJZB\Anaconda3\lib\site-packages\statsmodels\tsa\tsatools.py:142: FutureWarning: In a future version of pandas all arguments of concat except for the argument 'objs' will be keyword-only
      x = pd.concat(x[::order], 1)





<table class="simpletable">
<caption>OLS Regression Results</caption>
<tr>
  <th>Dep. Variable:</th>           <td>薪水</td>        <th>  R-squared:         </th> <td>   0.855</td>
</tr>
<tr>
  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.854</td>
</tr>
<tr>
  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   578.5</td>
</tr>
<tr>
  <th>Date:</th>             <td>Tue, 01 Feb 2022</td> <th>  Prob (F-statistic):</th> <td>6.69e-43</td>
</tr>
<tr>
  <th>Time:</th>                 <td>17:04:05</td>     <th>  Log-Likelihood:    </th> <td> -930.83</td>
</tr>
<tr>
  <th>No. Observations:</th>      <td>   100</td>      <th>  AIC:               </th> <td>   1866.</td>
</tr>
<tr>
  <th>Df Residuals:</th>          <td>    98</td>      <th>  BIC:               </th> <td>   1871.</td>
</tr>
<tr>
  <th>Df Model:</th>              <td>     1</td>      <th>                     </th>     <td> </td>   
</tr>
<tr>
  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   
</tr>
</table>
<table class="simpletable">
<tr>
    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>const</th> <td> 1.014e+04</td> <td>  507.633</td> <td>   19.981</td> <td> 0.000</td> <td> 9135.751</td> <td> 1.12e+04</td>
</tr>
<tr>
  <th>工龄</th>    <td> 2497.1513</td> <td>  103.823</td> <td>   24.052</td> <td> 0.000</td> <td> 2291.118</td> <td> 2703.185</td>
</tr>
</table>
<table class="simpletable">
<tr>
  <th>Omnibus:</th>       <td> 0.287</td> <th>  Durbin-Watson:     </th> <td>   0.555</td>
</tr>
<tr>
  <th>Prob(Omnibus):</th> <td> 0.867</td> <th>  Jarque-Bera (JB):  </th> <td>   0.463</td>
</tr>
<tr>
  <th>Skew:</th>          <td> 0.007</td> <th>  Prob(JB):          </th> <td>   0.793</td>
</tr>
<tr>
  <th>Kurtosis:</th>      <td> 2.667</td> <th>  Cond. No.          </th> <td>    9.49</td>
</tr>
</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.




**如果设置成一元二次方程，来看下模型评估效果**


```python
from sklearn.preprocessing import PolynomialFeatures
poly_reg = PolynomialFeatures(degree=2)
X_ = poly_reg.fit_transform(X)

import statsmodels.api as sm
X2 = sm.add_constant(X_)  # 这里传入的是含有x^2的X_
est = sm.OLS(Y, X2).fit()
est.summary()  # 在非Jupyter Notebook的编辑器中需要写成print(est.summary())
```




<table class="simpletable">
<caption>OLS Regression Results</caption>
<tr>
  <th>Dep. Variable:</th>           <td>薪水</td>        <th>  R-squared:         </th> <td>   0.931</td>
</tr>
<tr>
  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.930</td>
</tr>
<tr>
  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   654.8</td>
</tr>
<tr>
  <th>Date:</th>             <td>Tue, 01 Feb 2022</td> <th>  Prob (F-statistic):</th> <td>4.70e-57</td>
</tr>
<tr>
  <th>Time:</th>                 <td>17:04:28</td>     <th>  Log-Likelihood:    </th> <td> -893.72</td>
</tr>
<tr>
  <th>No. Observations:</th>      <td>   100</td>      <th>  AIC:               </th> <td>   1793.</td>
</tr>
<tr>
  <th>Df Residuals:</th>          <td>    97</td>      <th>  BIC:               </th> <td>   1801.</td>
</tr>
<tr>
  <th>Df Model:</th>              <td>     2</td>      <th>                     </th>     <td> </td>   
</tr>
<tr>
  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   
</tr>
</table>
<table class="simpletable">
<tr>
    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>const</th> <td> 1.399e+04</td> <td>  512.264</td> <td>   27.307</td> <td> 0.000</td> <td>  1.3e+04</td> <td>  1.5e+04</td>
</tr>
<tr>
  <th>x1</th>    <td> -743.6808</td> <td>  321.809</td> <td>   -2.311</td> <td> 0.023</td> <td>-1382.383</td> <td> -104.979</td>
</tr>
<tr>
  <th>x2</th>    <td>  400.8040</td> <td>   38.790</td> <td>   10.333</td> <td> 0.000</td> <td>  323.816</td> <td>  477.792</td>
</tr>
</table>
<table class="simpletable">
<tr>
  <th>Omnibus:</th>       <td> 2.440</td> <th>  Durbin-Watson:     </th> <td>   1.137</td>
</tr>
<tr>
  <th>Prob(Omnibus):</th> <td> 0.295</td> <th>  Jarque-Bera (JB):  </th> <td>   2.083</td>
</tr>
<tr>
  <th>Skew:</th>          <td>-0.352</td> <th>  Prob(JB):          </th> <td>   0.353</td>
</tr>
<tr>
  <th>Kurtosis:</th>      <td> 3.063</td> <th>  Cond. No.          </th> <td>    102.</td>
</tr>
</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.




可以看到模型效果的确有所提升

**补充知识点：另一种获取R-squared值的代码实现**


```python
# 1.读取数据
import pandas
df = pandas.read_excel('IT行业收入表.xlsx')
X = df[['工龄']]
Y = df['薪水']

# 2.模型训练
from sklearn.linear_model import LinearRegression
regr = LinearRegression()
regr.fit(X,Y)
```




    LinearRegression()




```python
from sklearn.metrics import r2_score
r2 = r2_score(Y, regr.predict(X))
print(r2)
```

    0.8551365584870814


可以看到和之前通过statsmodels库评估的结果是一致的。


```python

```
##  多元线性回归

###  多元线性回归的数学原理和代码实现

###  案例实战: 客户价值预测模型

**1. 案例背景**

这里以信用卡客户的客户价值来解释下客户价值预测的具体含义：客户价值预测就是指客户未来一段时间能带来多少利润，其利润的来源可能来自于信用卡的年费、取现手续费、分期手续费、境外交易手续费用等。而分析出客户的价值后，在进行营销、电话接听、催收、产品咨询等各项服务时，就可以针对高价值的客户进行区别于普通客户的服务，有助于进一步挖掘这些高价值客户的价值，并提高这些高价值客户的忠诚度。

**2. 读取数据**


```python
import pandas as pd
df = pd.read_excel('客户价值数据表.xlsx')
df.head()  # 显示前5行数据
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }


    .dataframe tbody tr th {
        vertical-align: top;
    }
    
    .dataframe thead th {
        text-align: right;
    }

</style>

<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>客户价值</th>
      <th>历史贷款金额</th>
      <th>贷款次数</th>
      <th>学历</th>
      <th>月收入</th>
      <th>性别</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1150</td>
      <td>6488</td>
      <td>2</td>
      <td>2</td>
      <td>9567</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1157</td>
      <td>5194</td>
      <td>4</td>
      <td>2</td>
      <td>10767</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1163</td>
      <td>7066</td>
      <td>3</td>
      <td>2</td>
      <td>9317</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>983</td>
      <td>3550</td>
      <td>3</td>
      <td>2</td>
      <td>10517</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>1205</td>
      <td>7847</td>
      <td>3</td>
      <td>3</td>
      <td>11267</td>
      <td>1</td>
    </tr>
  </tbody>
</table>

</div>




```python
X = df[['历史贷款金额', '贷款次数', '学历', '月收入', '性别']]
Y = df['客户价值']
```

**3. 模型搭建**


```python
from sklearn.linear_model import LinearRegression
regr = LinearRegression()
regr.fit(X,Y)
```




    LinearRegression()



**4. 线性回归方程构造**


```python
regr.coef_
```




    array([5.71421731e-02, 9.61723492e+01, 1.13452022e+02, 5.61326459e-02,
           1.97874093e+00])




```python
print('各系数为:' + str(regr.coef_))
print('常数项系数k0为:' + str(regr.intercept_))
```

    各系数为:[5.71421731e-02 9.61723492e+01 1.13452022e+02 5.61326459e-02
     1.97874093e+00]
    常数项系数k0为:-208.4200407995461


其中这里通过regr.coef_获得是一个系数列表，分别对应不同特征变量前面的系数，也即k1、k2、k3、k4及k5，所以此时的多元线性回归曲线方程为：

$y = -208 + 0.057x_1 + 96x_2 + 113x_3 + 0.056x_4 + 1.97x_5$

**5. 模型评估**


```python
import statsmodels.api as sm  # 引入线性回归模型评估相关库
X2 = sm.add_constant(X)
est = sm.OLS(Y, X2).fit()
est.summary()
```

    C:\Users\LYJZB\Anaconda3\lib\site-packages\statsmodels\tsa\tsatools.py:142: FutureWarning: In a future version of pandas all arguments of concat except for the argument 'objs' will be keyword-only
      x = pd.concat(x[::order], 1)





<table class="simpletable">
<caption>OLS Regression Results</caption>
<tr>
  <th>Dep. Variable:</th>          <td>客户价值</td>       <th>  R-squared:         </th> <td>   0.571</td>
</tr>
<tr>
  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.553</td>
</tr>
<tr>
  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   32.44</td>
</tr>
<tr>
  <th>Date:</th>             <td>Tue, 01 Feb 2022</td> <th>  Prob (F-statistic):</th> <td>6.41e-21</td>
</tr>
<tr>
  <th>Time:</th>                 <td>17:12:44</td>     <th>  Log-Likelihood:    </th> <td> -843.50</td>
</tr>
<tr>
  <th>No. Observations:</th>      <td>   128</td>      <th>  AIC:               </th> <td>   1699.</td>
</tr>
<tr>
  <th>Df Residuals:</th>          <td>   122</td>      <th>  BIC:               </th> <td>   1716.</td>
</tr>
<tr>
  <th>Df Model:</th>              <td>     5</td>      <th>                     </th>     <td> </td>   
</tr>
<tr>
  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   
</tr>
</table>
<table class="simpletable">
<tr>
     <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>const</th>  <td> -208.4200</td> <td>  163.810</td> <td>   -1.272</td> <td> 0.206</td> <td> -532.699</td> <td>  115.859</td>
</tr>
<tr>
  <th>历史贷款金额</th> <td>    0.0571</td> <td>    0.010</td> <td>    5.945</td> <td> 0.000</td> <td>    0.038</td> <td>    0.076</td>
</tr>
<tr>
  <th>贷款次数</th>   <td>   96.1723</td> <td>   25.962</td> <td>    3.704</td> <td> 0.000</td> <td>   44.778</td> <td>  147.567</td>
</tr>
<tr>
  <th>学历</th>     <td>  113.4520</td> <td>   37.909</td> <td>    2.993</td> <td> 0.003</td> <td>   38.406</td> <td>  188.498</td>
</tr>
<tr>
  <th>月收入</th>    <td>    0.0561</td> <td>    0.019</td> <td>    2.941</td> <td> 0.004</td> <td>    0.018</td> <td>    0.094</td>
</tr>
<tr>
  <th>性别</th>     <td>    1.9787</td> <td>   32.286</td> <td>    0.061</td> <td> 0.951</td> <td>  -61.934</td> <td>   65.891</td>
</tr>
</table>
<table class="simpletable">
<tr>
  <th>Omnibus:</th>       <td> 1.597</td> <th>  Durbin-Watson:     </th> <td>   2.155</td>
</tr>
<tr>
  <th>Prob(Omnibus):</th> <td> 0.450</td> <th>  Jarque-Bera (JB):  </th> <td>   1.538</td>
</tr>
<tr>
  <th>Skew:</th>          <td> 0.264</td> <th>  Prob(JB):          </th> <td>   0.464</td>
</tr>
<tr>
  <th>Kurtosis:</th>      <td> 2.900</td> <th>  Cond. No.          </th> <td>1.28e+05</td>
</tr>
</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 1.28e+05. This might indicate that there are<br/>strong multicollinearity or other numerical problems.





```python

```
